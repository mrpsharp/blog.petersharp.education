{
  
    
        "post0": {
            "title": "Benford's Law",
            "content": ". Tip: Click &quot;Launch Binder&quot; above to open a functioning version of the notebook in binder or &quot;View in Github&quot; to download to your own machine. Note the binder instance may take some time to load. . Someone recently mentioned to me that in physics many examples of numerical data have a high likelihood that the intial digit is small. This is known as Benford&#39;s Law. The explanation isn&#39;t clear, but it does require numbers to have a wide spread in magnitudes. Even then this article makes it clear that any simple arguments are suspect. . This feels like something worth exploring! . Benford&#39;s Law gives the probabilities of the leading digit $d$ as $$ P(d) = log_{10}(d+1) - log_{10}(d) $$ . benford = np.zeros(10) benford[0] = np.nan for d in range(1,10): benford[d] = np.log10(1+1/d) print(&quot;Leading Digit Probability&quot;) for d in range(1,10): print(f&quot;{d:^14}{benford[d]:^14.2%}&quot;) . . Leading Digit Probability 1 30.10% 2 17.61% 3 12.49% 4 9.69% 5 7.92% 6 6.69% 7 5.80% 8 5.12% 9 4.58% . This is a simple function to create a list of leading digits from a list of numbers . def find_leads(vals): leads = [] for val in vals: while (val &gt;= 10): val = val // 10 leads.append(int(val)) return leads . The first trial will generate a uniform distribution of numbers between 100 and 10,000. They will then randomly evolve by multiplying by a random value between 0.9 and 1.1 and the distribution of first digits monitored. . vals = [np.random.uniform(100,10000,1000)] # calculate the changes variation = &quot;geometric&quot; for x in range(0,400): new_vals = [] for val in vals[-1]: new_vals.append(val*np.random.uniform(0.9,1.1)) vals.append(new_vals) # Calculate the leading digits all_leads = [] for t in range(0,len(vals)): leads = find_leads(vals[t]) data = Counter(leads) # data[&#39;t&#39;] = t all_leads.append(data) . Plotting the evolution... . import matplotlib.pyplot as plt import matplotlib.animation as animation from IPython import display . . fig, ax = plt.subplots(figsize=(4, 2.5), dpi=144) def animate(i): ax.clear() # bar.set_data(all_leads[i].keys(), all_leads[i].values()) ax.bar(all_leads[i].keys(), all_leads[i].values()) ax.plot(np.arange(1,10,1),1000*benford[1:], linestyle=&quot;&quot;, marker=&quot;d&quot;,color=&#39;r&#39;, label=&quot;Benford Probability&quot;) ax.set_title(&quot;Iterations: &quot; + str(i)) ax.set_xticks(np.arange(1,10)) ax.set_ylim(0,330) ax.legend() anim = animation.FuncAnimation(fig, animate, interval=10,frames=400) plt.show() # writervideo = animation.FFMpegWriter(fps=40) # anim.save(&#39;benford2.mp4&#39;, writer=writervideo) # plt.close() . This shows a fairly rapid progression to Benfold&#39;s law - it works! . Next trial involves an arithmetic modification: adding a number between -20 and 20 each time the process cycles. . # Initial values randomly distributed vals2 = [np.random.uniform(100,10000,1000)] # calculate the changes variation = &quot;geometric&quot; for x in range(0,400): global vals2 new_vals = [] for val in vals2[-1]: change = np.random.uniform(-20,20) new_vals.append(abs(val+change)) vals2.append(new_vals) # Calculate the leading digits all_leads = [] for t in range(0,len(vals2)): leads = find_leads(vals2[t]) data = Counter(leads) # data[&#39;t&#39;] = t all_leads.append(data) fig, ax = plt.subplots(figsize=(4, 2.5), dpi=144) def animate(i): ax.clear() # bar.set_data(all_leads[i].keys(), all_leads[i].values()) ax.bar(all_leads[i].keys(), all_leads[i].values()) ax.plot(np.arange(1,10,1),1000*benford[1:], linestyle=&quot;&quot;, marker=&quot;d&quot;,color=&#39;r&#39;, label=&quot;Benford Probability&quot;) ax.set_title(&quot;Iterations: &quot; + str(i)) ax.set_xticks(np.arange(1,10)) ax.set_ylim(0,330) ax.legend() anim = animation.FuncAnimation(fig, animate, interval=10,frames=400) plt.show() # writervideo = animation.FFMpegWriter(fps=40) # anim.save(&#39;benford2.mp4&#39;, writer=writervideo) # plt.close() . . This shows no particular evolution to Benfold. We can compare the distribution of the numbers at the end of 400 iterations. . fig, axs = plt.subplots(1, 2, tight_layout=True) axs[0].hist(vals[400], 20) axs[0].set_title(&quot;Geometric variation&quot;) axs[1].hist(vals2[400], 20) axs[1].set_title(&quot;Arithmetic variation&quot;) plt.show() . . The respective maximum values are: . max1 = max(vals[400]) max2 = max(vals2[400]) print(f&quot;{max1:.3} and {max2:.3}&quot;) . 1.06e+05 and 1.04e+04 . It may make more sense to look at the histograms on a log scale. . fig, axs = plt.subplots(1, 2, tight_layout=True) logbins = np.logspace(np.log10(min(vals[400])),np.log10(max(vals[400])),20) logbins2 = np.logspace(np.log10(min(vals2[400])),np.log10(max(vals2[400])),20) axs[0].hist(vals[400], bins=logbins) axs[0].set_title(&quot;Geometric variation&quot;) axs[0].set_xscale(&#39;log&#39;) axs[1].hist(vals2[400], bins=logbins2) axs[1].set_title(&quot;Arithmetic variation&quot;) axs[1].set_xscale(&#39;log&#39;) plt.show() . It is clear that the geometric variation gives much even more spread on a log scale. This provides a good visual explanation for Benfold&#39;s Law: picking a random point on that log scale and you are most likely to land in and interval beginning with 1 (the wide intervals just after each $10^x$ label on the axis. The arithmetic variation curve only really spans one order of magnitude so this effect is much less pronounced. Running the process longer and increasing each step-size for the arithmetic variation may help. . from fastprogress.fastprogress import progress_bar # Initial values randomly distributed vals3 = np.random.uniform(100,10000,1000) # calculate the changes n = 50000 # number of iterations - change this to explore for x in progress_bar(range(0,n)): global vals3 new_vals = [] for val in vals3: change = np.random.uniform(-150,150) new_vals.append(abs(val+change)) vals3=new_vals # Calculate the leading digits leads = find_leads(vals3) all_leads = Counter(leads) fig, ax = plt.subplots(1,2, tight_layout=True) ax[0].bar(all_leads.keys(), all_leads.values()) ax[0].plot(np.arange(1,10,1),1000*benford[1:], linestyle=&quot;&quot;, marker=&quot;d&quot;,color=&#39;r&#39;, label=&quot;Benford Probability&quot;) ax[0].set_title(f&quot;Iterations: {n:,}&quot;) ax[0].set_xticks(np.arange(1,10)) ax[0].legend() logbins3 = np.logspace(np.log10(min(vals3)),np.log10(max(vals3)),20) ax[1].hist(vals3, bins=logbins3) ax[1].set_title(&quot;Arithmetic variation&quot;) ax[1].set_xscale(&#39;log&#39;) plt.show() . . . 100.00% [50000/50000 02:04&lt;00:00] It looks like it&#39;s heading in the right direction now that it spans two orders of magnitude, but it isn&#39;t completely convincing... . Another option is simply to investigate some distributions directly looking at the same two charts. . def benfold_distrb(vals, distname): # Calculate the leading digits leads = find_leads(vals) all_leads = Counter(leads) fig, ax = plt.subplots(1,2, tight_layout=True) ax[0].bar(all_leads.keys(), all_leads.values()) ax[0].plot(np.arange(1,10,1),1000*benford[1:], linestyle=&quot;&quot;, marker=&quot;d&quot;,color=&#39;r&#39;, label=&quot;Benford Probability&quot;) ax[0].set_title(distname) ax[0].set_xticks(np.arange(1,10)) ax[0].legend() logbins = np.logspace(np.log10(min(vals)),np.log10(max(vals)),20) ax[1].hist(vals, bins=20) # ax[1].set_xscale(&#39;log&#39;) plt.show() . . normal = np.random.default_rng().normal(1e7, 2.5e6, 1000) benfold_distrb(normal, &quot;Normal&quot;) . Well, that&#39;s weird. Something to do with the sampling method maybe?? Any ideas in the comments! .",
            "url": "https://blog.petersharp.education/2022/08/27/benfords-law/",
            "relUrl": "/2022/08/27/benfords-law/",
            "date": " • Aug 27, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://blog.petersharp.education/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.petersharp.education/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}